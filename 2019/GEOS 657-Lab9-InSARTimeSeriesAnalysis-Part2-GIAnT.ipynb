{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h1>\n",
    "<b>WARNING:</b> This notebook has been deprecated and no longer runs in an OSL supported conda environment.\n",
    "</h1>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.png\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing<b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 9: InSAR Time Series Analysis using GIAnT within Jupyter Notebooks<br>Part 2: GIAnT <font color='rgba(200,0,0,0.2)'> -- [## Points] </font> </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer & Joshua J C Knicely; University of Alaska Fairbanks</b> <br>\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" /><font color='rgba(200,0,0,0.2)'> <b>Due Date: </b>NONE</font>\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. This lab is divided into 3 parts: 1) data download and preprocessing, 2) GIAnT time series, and 3) a simple Mogi source inversion. The primary goal of this lab is to demonstrate how to process InSAR data, specifically interferograms, using the Generic InSAR Analysis Toolbox (<a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">GIAnT</a>) in the framework of *Jupyter Notebooks*.<br>\n",
    "\n",
    "<b>Our specific objectives for this lab are to:</b>\n",
    "\n",
    "- Prepare data for GIAnT: \n",
    "    - Create necessary ancillary files and functions. \n",
    "    - Reformat the interferograms. \n",
    "- Run GIAnT\n",
    "- Use GIAnT to create maps of surface deformation: \n",
    "    -  Understand its capabilities. \n",
    "    -  Understand its limitations. \n",
    "</font>\n",
    "\n",
    "<br>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> Target Description </b> </font>\n",
    "\n",
    "<font size=\"4\"> <font color='rgba(200,0,0,0.2)'> <b>THIS NOTEBOOK INCLUDES NO HOMEWORK ASSIGNMENTS.</b></font> <br>\n",
    "\n",
    "Contact me at fjmeyer@alaska.edu should you run into any problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "notebookUrl = notebookUrl.value\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/rtc_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"rtc_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select \"rtc_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"rtc_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>Overview</b></font>\n",
    "<br>\n",
    "<font size='3'><b>About GIAnT</b>\n",
    "<br>\n",
    "GIAnT is a Python framework that allows rapid time series analysis of low amplitude deformation signals. It allows users to use multiple time series analysis technqiues: Small Baseline Subset (SBAS), New Small Baseline Subset (N-SBAS), and Multiscale InSAR Time-Series (MInTS). As a part of this, it includes the ability to correct for atmospheric delays by assuming a spatially uniform stratified atmosphere. \n",
    "<br><br>\n",
    "<b>Limitations</b>\n",
    "<br>\n",
    "GIAnT has a number of limitations that are important to keep in mind as these can affect its effectiveness for certain applications. It implements the simplest time-series inversion methods. Its single coherence threshold is very conservative in terms of pixel selection. It does not include any consistency checks for unwrapping errors. It has a limited dictionary of temporal model functions. It cannot correct for atmospheric effects due to differing surface elevations. \n",
    "<br><br>\n",
    "<b>Steps to use GIAnT</b><br>\n",
    "Although GIAnT is an incredibly powerful tool, it requires very specific input. Because of the input requirements, the bulk of the lab and code below is dedicated to getting our data into a form that GIAnT can manipulate and to creating files that tell GIAnT what to do. The general steps to use GIAnT are below. \n",
    "<br><br>\n",
    "More information about GIAnT can be found here, <a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">http://earthdef.caltech.edu/projects/giant/wiki</a>.\n",
    "<br><br>\n",
    "- Download Data\n",
    "- Identify Area of Interest\n",
    "- Subset (Crop) Data to Area of Interest\n",
    "- Prepare Data for GIAnT (Today's lab starts here)\n",
    "    - Adjust file names\n",
    "    - Remove potentially disruptive default values (optional)\n",
    "    - Convert data from '.tiff' to '.flt' format\n",
    "- Create Input Files for GIAnT\n",
    "    - Create 'ifg.list'\n",
    "    - Create 'date.mli.par'\n",
    "    - Make prepxml_SBAS.py\n",
    "    - Run prepxml_SBAS.py\n",
    "    - Make userfn.py\n",
    "- Run GIAnT\n",
    "    - PrepIgramStack.py\n",
    "    - ProcessStack.py\n",
    "    - SBASInvert.py\n",
    "    - SBASxval.py\n",
    "- Data Visualization\n",
    "\n",
    "<br><br>\n",
    "When you use GIAnT, cite the creator's work using:<br>\n",
    "Agram et al., (2013). \"New Radar Interferometric Time Series Analysis Toolbox Released.\" Eos Trans. AGU, 94, 69. \n",
    "<br>AND<br>\n",
    "Agram et al., (2012). \"Generic InSAR Analysis Toolbox (GIAnT) - User Guide.\" <http://earthdef.caltech.edu>\n",
    "<br><br><b><i>DO WE NEED TO ALSO GIVE ASF A CITATION???</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>Required Files</b></font>\n",
    "<br>\n",
    "<font size='3'>Before we begin, each student should have a specific subset of files with a specific folder structure as described below. If your folder structure differs, the code will need to be modified. \n",
    "<br>\n",
    "- Folder: Corrected Subsets\n",
    "    - Files: TRAIN corrected unwrapped phase subsets\n",
    "    - Files: Uncorrected unwrapped phase subsets\n",
    "- Folder: Ingram Subsets\n",
    "    - Files: Amplitude subsets\n",
    "    - Files: Coherence subsets\n",
    "    - Files: Unwrapped phase (ingram) subsets\n",
    "        - Theoretically, these are identical to the uncorrected unwrapped phase subsets in the Corrected Subsets folder. \n",
    "    \n",
    "</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>0 System Setup</b></font><br>\n",
    "    <font size='3'>We will first do some system setup. This involves importing requiesite Python libraries and defining all of our user inputs. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>0.0 Import Python Packages</b></font><br>\n",
    "    <font size='3'>Let's import the Python libraries we will need to run this lab. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import h5py\n",
    "import shutil\n",
    "import re\n",
    "from datetime import date\n",
    "import glob\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from math import ceil\n",
    "\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import matplotlib.patches as patches  # for Rectangle\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "import numpy as np\n",
    "from numpy import isneginf\n",
    "\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "import opensarlab_lib as asfn\n",
    "asfn.jupytertheme_matplotlib_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>0.1 Enter an analysis directory prepared in the Part 1 notebook</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_pth = \"part1_pickle\"\n",
    "while True:\n",
    "    analysis_directory = input(\"Enter the absolute path to the directory holding your data from Part 1:\\n\")\n",
    "    if os.path.exists(f\"{analysis_directory}/{pickle_pth}\"):\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\n{analysis_directory} does not contain {pickle_pth}\")\n",
    "        continue\n",
    "os.chdir(analysis_directory)\n",
    "print(f\"\\nanalysis_directory: {analysis_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>0.2 Open our pickle from the Part 1 notebook</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_pth = \"part1_pickle\"\n",
    "if os.path.exists(pickle_pth):\n",
    "    with open(pickle_pth,'rb') as infile:\n",
    "        part1_pickle = pickle.load(infile)\n",
    "    print(f\"part1_pickle = {part1_pickle}\")\n",
    "else:\n",
    "    print('Invalid Path. Did you follow part 1 and generate your pickle?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>0.3 Define some important paths</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingram_folder = part1_pickle['ingram_folder'] # Location of the original ingram folders\n",
    "corrected_folder = part1_pickle['corrected_folder'] # Location of the TRAIN corrected subsets\n",
    "subset_folder = part1_pickle['subset_folder'] # Location of the original (uncorrected) subsets\n",
    "giant_dir = f'{analysis_directory}/GIAnT' # directory where we will perform GIAnT analysis\n",
    "giant_data = f'{analysis_directory}/GIAnT_Data' # directory where we will store our data for use by GIAnT\n",
    "output_dir = f'{analysis_directory}/geotiffs_and_animations'\n",
    "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>0.4 Decide whether or not to delete old data</b></font>\n",
    "<br>\n",
    "<font size='3'>True = delete\n",
    "<br>\n",
    "False = save</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_giant_dir = True # True or False; if True, any folder with the same name will be deleted and recreated. \n",
    "replace_giant_data = True\n",
    "replace_output_dir = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set things to find files\n",
    "unw = '_unw_phase.'\n",
    "unw_corr = '_unw_phase_corrected.'\n",
    "corr = '_corr.'\n",
    "amp = '_amp.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>0.5 Delete the GIAnT data and working directories if 'replace_* = True'</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_giant_dir:\n",
    "    try:\n",
    "        shutil.rmtree(giant_dir)\n",
    "    except:\n",
    "        pass\n",
    "if replace_giant_data:\n",
    "    try: \n",
    "        shutil.rmtree(giant_data)\n",
    "    except:\n",
    "        pass\n",
    "if replace_output_dir:\n",
    "    try:\n",
    "        shutil.rmtree(output_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# If the GIAnT data and working directories don't exist, create them. \n",
    "pathlib.Path(giant_dir).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(giant_data).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>0.7 Load the pickled heading_avg</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_avg = part1_pickle['heading_avg']\n",
    "print(f\"heading_avg = {heading_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>0.8 Copy a single clipped geotiff into our working GIAnT directory for later data visualization</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amps = [f for f in os.listdir(subset_folder) if '_amp' in f]\n",
    "amps.sort()\n",
    "shutil.copy(os.path.join(subset_folder, amps[0]), giant_dir)\n",
    "radar_tiff = os.path.join(giant_dir, amps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>1. Create Input Files And Code for GIAnT</b></font>\n",
    "    <br>\n",
    "    <font size ='3'>Let's create the input files and specialty code that GIAnT requires. These are listed below. \n",
    "        <br>\n",
    "        \n",
    "- ifg.list\n",
    "    - List of the interferogram properties including primary and secondary dates, perpendicular baseline, and sensor name. \n",
    "- date.mli.par\n",
    "    - File from which GIAnT pulls requisite information about the sensor. \n",
    "    - This is specifically for GAMMA files. When using other interferogram processing techniques, an alternate file is required. \n",
    "- prepxml_SBAS.py\n",
    "    - Python function to create an xml file that specifies the processing options to GIAnT. \n",
    "    - This must be modified by the user for their particular application. \n",
    "- userfn.py\n",
    "    - Python function to map the interferogram dates to a phyiscal file on disk. \n",
    "    - This must be modified by the user for their particular application. \n",
    "    </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>1.1 Create 'ifg.list' File </b></font>\n",
    "<br><br>\n",
    "<font face='Calibri' size='3'><b>1.1.0 Create a list of unwrapped, phase-corrected data</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(corrected_folder) if f.endswith(f'{unw_corr}tif')] # Just get one of each file name. \n",
    "files.sort()\n",
    "print(len(files))\n",
    "print(*files, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>1.1.1 Create a 4 column text file to communicate network information to GIAnT within the GIAnT folder</b>\n",
    "<br>\n",
    "Contains primary date, secondary date, perpendicular baseline, and sensor designation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dates = []\n",
    "secondary_dates = []\n",
    "\n",
    "for file in files:\n",
    "    reference_dates.append(file[0:8])\n",
    "    secondary_dates.append(file[9:17])\n",
    "# Sort the dates according to the primary dates. \n",
    "p_dates, s_dates = (list(t) for t in zip(*sorted(zip(reference_dates, secondary_dates))))\n",
    "\n",
    "with open(os.path.join(giant_dir, 'ifg.list'), 'w') as fid:\n",
    "    for i, dt in enumerate(p_dates):\n",
    "        primary_date = dt # pull out Primary Date (first set of numbers)\n",
    "        secondary_date = s_dates[i] # pull out Secondary Date (second set of numbers)\n",
    "        bperp = '0.0' # perpendicular baseline. \n",
    "        sensor = 'S1' # Sensor designation. \n",
    "        \n",
    "        # write values to the 'ifg.list' file.\n",
    "        fid.write(f'{primary_date}  {secondary_date}  {bperp}  {sensor}\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>1.1.2 Print ifg.list</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(giant_dir, 'ifg.list'), 'r') as fid: \n",
    "    print(fid.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>You may notice that the code above sets the perpendicular baseline to a value of 0.0 m. This is not the true perpendicular baseline. That value can be found in metadata file (titled <font face='Courier New'>$<$primary timestamp$>$_$<$secondary timestamp$>$.txt</font>) that comes with the original interferogram. Generally, we would want the true baseline for each interferogram. However, since Sentinel-1 has such a short baseline, a value of 0.0 m is sufficient for our purposes. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'> <b>1.2 Create 'date.mli.par' File </b></font> \n",
    "<br>\n",
    "<font face='Calibri' font size='3'> As we are using GAMMA products, we must create a 'date.mli.par' file from which GIAnT will pull necessary information. If another processing technique is used to create the interferograms, an alternate file name and file inputs are required. \n",
    "<br><br>\n",
    "<b>1.2.0 Make a list of paths to all of the unwrapped interferograms</b>\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(corrected_folder) if f.endswith(f'{unw_corr}tif')]\n",
    "print(len(files))\n",
    "print(*files, sep=\"\\n\")\n",
    "\n",
    "print(f'\\n\\nCurrent Working Directory: {os.getcwd()}')\n",
    "print(f'radar_tiff path: {radar_tiff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' font size='3'><b>1.2.1 Gather metadata required by GIAnT and save it to a date.mli.par file</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_insar_aquisition_date(ingram_name):\n",
    "    regex = '[0-9]{8}T[0-9]{6}'\n",
    "    match = re.search(regex, ingram_name)\n",
    "    if match:\n",
    "        return match[0]\n",
    "\n",
    "ds = gdal.Open(radar_tiff, gdal.GA_ReadOnly)\n",
    "\n",
    "# Get WIDTH (xsize; AKA 'Pixels') and FILE_LENGTH (ysize; AKA 'Lines') information\n",
    "n_lines = ds.RasterYSize\n",
    "n_pixels = ds.RasterXSize\n",
    "ds = None\n",
    "\n",
    "# Get the center line UTC time stamp; can also be found inside <date>_<date>.txt file\n",
    "ingram_name = os.listdir(ingram_folder)[0]\n",
    "tstamp = get_primary_insar_aquisition_date(ingram_name)\n",
    "c_l_utc = int(tstamp[0:2])*3600 + int(tstamp[2:4])*60 + int(tstamp[4:6])\n",
    "\n",
    "# radar frequency; speed of light divided by radar wavelength of Sentinel-1 in meters\n",
    "rfreq = 299792548.0 / 0.055465763 \n",
    "\n",
    "# write the 'date.mli.par' file\n",
    "with open(os.path.join(giant_dir, 'date.mli.par'), 'w') as fid:\n",
    "    # when using GAMMA products, GIAnT requires the radar frequency. Everything else is in wavelength (m) \n",
    "    fid.write(f'radar_frequency: {rfreq} \\n') \n",
    "    # Method from Tom Logan's prepGIAnT code; can also be found inside <date>_<date>.txt file\n",
    "    fid.write(f'center_time: {c_l_utc} \\n') \n",
    "    # inside <date>_<date>.txt file; can be hardcoded or set up so code finds it. \n",
    "    fid.write(f'heading: {heading_avg} \\n')\n",
    "    # number of lines in direction of the satellite's flight path\n",
    "    fid.write(f'azimuth_lines: {n_lines} \\n')\n",
    "    # number of pixels in direction perpendicular to satellite's flight path\n",
    "    fid.write(f'range_samples: {n_pixels} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' font size='3'><b>1.2.2 Print the metadata stored in 'date.mli.par'</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(giant_dir, 'date.mli.par'), 'r') as fid:\n",
    "    print(fid.read())                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>1.3 Create prepxml_SBAS.py Function</b> </font>\n",
    "<br>\n",
    "<font size='3'>We will create a prepxml_SBAS.py function and put it into our GIAnT working directory.</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='3'><b>Necessary prepxml_SBAS.py edits</b></font>\n",
    "<br>\n",
    "<font size='3'> GIAnT comes with an example prepxml_SBAS.py, but requries significant edits for our purposes. These alterations have already been made, so we don't have to do anything now, but it is good to know the kinds of things that have to be altered. The details of some of these options can be found in the GIAnT documentation. The rest must be found in the GIAnT processing files themselves, most notably the tsxml.py and tsio.py functions. <br>The following alterations were made:\n",
    "<br>\n",
    "- Changed 'example' &#9658; 'date.mli.par'\n",
    "- Removed 'xlim', 'ylim', 'rxlim', and 'rylim'\n",
    "    - These are used for clipping the files in GIAnT. As we have already done this, it is not necessary. \n",
    "- Removed latfile='lat.map' and lonfile='lon.map'\n",
    "    - These are optional inputs for the latitude and longitude maps. \n",
    "- Removed hgtfile='hgt.map'\n",
    "    - This is an optional altitude file for the sensor. \n",
    "- Removed inc=21.\n",
    "    - This is the optional incidence angle information. \n",
    "    - It can be a constant float value or incidence angle file. \n",
    "    - For Sentinel1, it varies from 29.1-46.0&deg;.\n",
    "- Removed masktype='f4'\n",
    "    - This is the mask designation. \n",
    "    - We are not using any masks for this. \n",
    "- Changed unwfmt='RMG' &#9658; unwfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Removed demfmt='RMG'\n",
    "- Changed corfmt='RMG' &#9658; corfmt='GRD'\n",
    "    - Read data using GDAL. \n",
    "- Changed nvalid=30 -> nvalid=1\n",
    "    - This is the minimum number of interferograms in which a pixel must be coherent. A particular pixel will be included only if its coherence is above the coherence threshold, cohth, in more than nvalid number of interferograms. \n",
    "- Removed atmos='ECMWF'\n",
    "    - This is an amtospheric correction command. It depends on a library called 'pyaps' developed for GIAnT. This library has not been installed yet. \n",
    "- Changed masterdate='19920604' &#9658; masterdate='20161119'\n",
    "    - Use our actual masterdate. \n",
    "    - I simply selected the earliest date as the masterdate.</font>\n",
    "<br><br>\n",
    "    <font size='3'><b>1.3.0 Define some prepxml inputs</b>\n",
    "        <br>\n",
    " The reference region and related variables</font></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ref_region = False\n",
    "filt = 1.0 / 12 # Start with a temporal filter in length of years\n",
    "ref_region_size = [5, 5]   # Reference region size in Lines and Pixels\n",
    "ref_region_center = [-0.9, -91.3] # Center of reference region in lat, lon coordinates\n",
    "rxlim = [0, 10]\n",
    "rylim = [95, 105]\n",
    "\n",
    "primary_date = min([amps[i][0:8] for i in range(len(amps))], key=int) # set the primary date.\n",
    "print(f\"Primary Date: {primary_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>1.3.1 Create the prepxml Python script</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ref_region == True:\n",
    "    use_ref = ''\n",
    "else:\n",
    "    use_ref = '#'\n",
    "\n",
    "prepxml_sbas_template = '''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Example script for creating XML files for use with the SBAS processing chain. This script is supposed to be copied to the working directory and modified as needed.\"\"\"\n",
    "\n",
    "import sys\n",
    "tsinsar_pth = '/home/jovyan/.local/GIAnT'\n",
    "if tsinsar_pth not in sys.path:\n",
    "    sys.path.append(tsinsar_pth)\n",
    "\n",
    "import tsinsar as ts\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def parse():\n",
    "    parser= argparse.ArgumentParser(description='Preparation of XML files for setting up the processing chain. Check tsinsar/tsxml.py for details on the parameters.')\n",
    "    parser.parse_args()\n",
    "\n",
    "parse()\n",
    "g = ts.TSXML('data')\n",
    "g.prepare_data_xml(\n",
    "    '{0}/date.mli.par', proc='GAMMA', \n",
    "    {7}rxlim = [{1},{2}], rylim=[{3},{4}],\n",
    "    inc = 21., cohth=0.10, \n",
    "    unwfmt='GRD', corfmt='GRD', chgendian='True', endianlist=['UNW','COR'])\n",
    "g.writexml('data.xml')\n",
    "\n",
    "\n",
    "g = ts.TSXML('params')\n",
    "g.prepare_sbas_xml(nvalid=1, netramp=True, \n",
    "    #atmos='NARR', \n",
    "    demerr=False, uwcheck=False, regu=True, masterdate='{5}', filt={6})\n",
    "g.writexml('sbas.xml')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Program is part of GIAnT v1.0                            #\n",
    "# Copyright 2012, by the California Institute of Technology#\n",
    "# Contact: earthdef@gps.caltech.edu                        #\n",
    "############################################################\n",
    "\n",
    "'''\n",
    "\n",
    "with open(os.path.join(giant_dir, 'prepxml_SBAS.py'), 'w') as fid:\n",
    "    fid.write(prepxml_sbas_template.format(giant_dir,\n",
    "                                           rxlim[0], rxlim[1],\n",
    "                                           rylim[0], rylim[1],\n",
    "                                           primary_date, filt,use_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>1.3.2 Print the prepxml script</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(giant_dir, 'prepxml_SBAS.py'), 'r') as fid:\n",
    "    print(fid.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>1.4 Create userfn.py script</b></font>\n",
    "<br>\n",
    "<font size='3'>This file maps the interferogram dates to a physical file on disk. This python file must be in our working directory, <b>/GIAnT</b>.\n",
    "<br><br>\n",
    "    <b>1.4.0 Write userfn.py</b>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userfn_template = \"\"\"\n",
    "#!/usr/bin/env python\n",
    "import os \n",
    "\n",
    "def makefnames(dates1, dates2, sensor):\n",
    "    dirname = '{0}'\n",
    "    root = os.path.join(dirname, dates1+'_'+dates2)\n",
    "    unwname = root+'{1}'+'flt' # for potentially disruptive default values removed. \n",
    "    corname = root+'_corr.flt'\n",
    "    return unwname, corname\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(giant_dir, 'userfn.py'), 'w') as fid:\n",
    "    fid.write(userfn_template.format(giant_data, unw_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>1.4.1 Print userfn.py</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(giant_dir, 'userfn.py'), 'r') as fid:\n",
    "    print(fid.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size='5'> <b> 2. Prepare Data for GIAnT </b> </font>\n",
    "<br>\n",
    "<font size='3'> Our resultant files need to be adjusted for input into GIAnT. This involves:\n",
    "    <ul>\n",
    "    <li>Adjusting filenames to start with the date</li>\n",
    "    <li>(Optional) Removing potentially disruptive default values</li>\n",
    "    <li>Converting data format from '.tif' to '.flt'</li>\n",
    "    </ul>\n",
    "</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>2.1 Adjust File Names</b></font>\n",
    "    <font size='3'><br>GIAnT requires the files to have a simple format of <font face='Courier New'>&lt;primary_date&gt;-&lt;secondary_date&gt;_&lt;unwrapped or coherence designation&gt;.tiff</font>.\n",
    "<br><br>\n",
    "<font color='green'><b>We already did this step in Part 1, so section 2.1 is just a reminder that it is also a requirement for GIAnT.</b></font>\n",
    "</font> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'><b>2.2 Remove disruptive default values</b></font> \n",
    "    <br>\n",
    "    <font size='3'>Now we can remove the potentially disruptive default values from the unwrapped interferograms. For this lab, this step is optional. This is because the Sentinel-1 data does not have any of the disruptive default values. However, for other datasources, this may be required.<br>This works by creating an entirely new geotiff with the text <font face='Courier New'>'\\_no\\_default'</font> added to the file name.\n",
    "<br><br>\n",
    "        <b>2.2.0 Write functions to gather and print paths via a wildcard path.</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_paths(paths):\n",
    "    tiff_paths = glob.glob(paths)\n",
    "    tiff_paths.sort()\n",
    "    return tiff_paths\n",
    "\n",
    "def print_tiff_paths(tiff_paths):\n",
    "    for p in tiff_paths:\n",
    "        print(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.2.1 Gather the paths to the uncorrected and corrected unw_phase tifs</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_unc = f\"{corrected_folder}/*{unw}tif\"\n",
    "paths_corr = f\"{corrected_folder}/*{unw_corr}tif\"\n",
    "files_unc = get_tiff_paths(paths_unc)\n",
    "files_corr = get_tiff_paths(paths_corr)\n",
    "\n",
    "print_tiff_paths(files_unc)\n",
    "print_tiff_paths(files_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.2.2 Decide if you wish to replace dispruptive NaN values with zeros</b>\n",
    "    <br>\n",
    "    True = replace NaNs\n",
    "    <br>\n",
    "    False = keep NaNs\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_nans = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.2.3 Write a function to remove disruptive (NaN) values</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_values(paths):\n",
    "    for file in paths:\n",
    "        outfile = os.path.join(analysis_directory, file.replace('.tif','_no_default.tif'))\n",
    "        cmd = f\"gdal_calc.py --calc=\\\"(A>-1000)*A\\\" --outfile={outfile} -A {analysis_directory}/{file} --NoDataValue=0 --format=GTIFF\"\n",
    "        subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='3'><b>2.2.4 Remove files that have potentially disruptive default values.</b> This step will run only if there is a file with '\\_no\\_default' in the <font face='Courier New' size='2'>corrected_folder</font> directory.</font> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if replace_nans:\n",
    "    replace_nan_values(files_unc)\n",
    "    replace_nan_values(files_corr)\n",
    "    # change 'unw' and 'unw_corr' so the appropriate files will still be found in later code. \n",
    "    unw = unw.replace('_unw_phase', '_unw_phase_no_default')\n",
    "    unw_corr = unw_corr.replace('_corrected', '_corrected_no_default')\n",
    "else:\n",
    "    print(\"Skipping the removal of disruptive values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size='4'><b>2.3 Convert data format from '.tif' to '.flt'</b><br></font>\n",
    "<font size='3'>This is a requirement of GIAnT\n",
    "<br><br>\n",
    "<b>2.3.0 Gather the paths to the files to convert</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unwrapped phase files to convert. \n",
    "paths_unc = f\"{corrected_folder}/*{unw}tif\"\n",
    "paths_corr = f\"{corrected_folder}/*{unw_corr}tif\"\n",
    "paths_cohr = f\"{subset_folder}/*{corr}tif\"\n",
    "files_unc = get_tiff_paths(paths_unc)\n",
    "files_corr = get_tiff_paths(paths_corr)\n",
    "files_cohr = get_tiff_paths(paths_cohr)\n",
    "\n",
    "# file format should appear as below\n",
    "# YYYYMMDD-YYYYMMDD_unw_phase.flt\n",
    "# OR, if default values have been removed...\n",
    "# YYYYMMDD-YYYYMMDD_unw_phase_no_default.flt\n",
    "\n",
    "print_tiff_paths(files_unc)\n",
    "print_tiff_paths(files_corr)\n",
    "print_tiff_paths(files_cohr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.3.1 Convert the .tif files to .flt files</b>\n",
    "    <br>\n",
    "tiff_to_flt creates 3 files: '*.flt', '*.flt.aux.xml', and '*.hdr', where the asterisk, '*', is replaced with the file name</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiff_to_flt(paths):\n",
    "    for file in paths:\n",
    "        newname, old_ext = os.path.splitext(file)\n",
    "        # Create system command that will convert the file from a .tiff to a .flt\n",
    "        cmd = f\"gdal_translate -of ENVI {file} {file.replace('.tif','.flt')}\"\n",
    "        try:\n",
    "            #print(f\"cmd = {cmd}\") # display what the command looks like. \n",
    "            # pass command to the system\n",
    "            subprocess.call(cmd, shell=True)\n",
    "        except: \n",
    "            print(f\"Problem with command: {cmd}\")\n",
    "    print(\"Conversion from '.tif' to '.flt' complete.\")\n",
    "    \n",
    "tiff_to_flt(files_unc)\n",
    "tiff_to_flt(files_corr)\n",
    "tiff_to_flt(files_cohr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = 'Calibri' size='3'><b>2.3.1 Gather the paths to the '\\*.flt', '\\*.hdr', and '\\*.aux.xml' files</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_flt = f\"*/*.flt\"\n",
    "paths_hdr = f\"*/*.hdr\"\n",
    "paths_aux = f\"*/*.flt.aux.xml\"\n",
    "\n",
    "files_flt = get_tiff_paths(paths_flt)\n",
    "files_hdr = get_tiff_paths(paths_hdr)\n",
    "files_aux = get_tiff_paths(paths_aux)\n",
    "\n",
    "print_tiff_paths(files_flt)\n",
    "print_tiff_paths(files_hdr)\n",
    "print_tiff_paths(files_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.3.2 Move the files and delete the extraneous '.flt.aux.xml' files</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_list_of_files(paths, new_analysis_directory):\n",
    "    for file in paths:\n",
    "        path, name = os.path.split(file)\n",
    "        try:\n",
    "            shutil.move(file,os.path.join(new_analysis_directory, name))\n",
    "        except FileNotFoundError as e:\n",
    "            raise type(e)(f\"Failed to find {file}\")\n",
    "\n",
    "move_list_of_files(files_flt, giant_data)\n",
    "move_list_of_files(files_hdr, giant_data)\n",
    "\n",
    "for file in files_aux:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>2.4 Run prepxml_SBAS.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> Here we run <b>prepxml_SBAS.py</b> to create the 2 needed files\n",
    "\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "\n",
    "To use MinTS, we would create and run <b>prepxml_MinTS.py</b> to create\n",
    "\n",
    "- data.xml\n",
    "- mints.xml\n",
    "        \n",
    "These files are needed by <b>PrepIgramStack.py</b>. \n",
    "</font>\n",
    "<br><br>\n",
    "<font size='3'><b>Define the path to the GIAnT module, change into giantDir, and run prepxml_SBAS.py</b>\n",
    "<br>\n",
    "Note: We have to change into giant_dir due to GIAnT's use of relative paths to find expected files.\n",
    "<br><br>\n",
    "<b>2.4.0 Define the path to the GIAnT installation and move into giant_dir (in your analysis directory)</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_path = \"/home/jovyan/.local/GIAnT/SCR\"\n",
    "\n",
    "#install cython (replaces weave. Needed for running GIAnT in Python 3)\n",
    "# !pip install --user cython\n",
    "\n",
    "try:\n",
    "    os.chdir(giant_dir)\n",
    "except FileNotFoundError as e:\n",
    "    raise type(e)(f\"Failed to find {file}\")\n",
    "    \n",
    "print(f\"current directory: {os.getcwd()}\\n\")\n",
    "for file in glob.glob('*.*'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.4.1 Download GIAnT from the `asf-jupyter-data-west` S3 bucket</b>\n",
    "    <br>\n",
    "    GIAnT is no longer supported (Python 2). This unofficial version of GIAnT has been partially ported to Python 3 to run this notebook. Only the portions of GIAnT used in this notebook have been tested.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/home/jovyan/.local/GIAnT\"):\n",
    "    download_path = 's3://asf-jupyter-data-west/GIAnT_5_21.zip'\n",
    "    output_path = f\"/home/jovyan/.local/{os.path.basename(download_path)}\"\n",
    "    !aws --region=us-west-2 --no-sign-request s3 cp $download_path $output_path\n",
    "    if os.path.isfile(output_path):\n",
    "        !unzip $output_path -d /home/jovyan/.local/\n",
    "        os.remove(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.4.2 Run prepxml_SBAS.py</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepxml_SBAS.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.4.3 View the newly created data.xml</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.xml', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.4.4 View the newly created 'sbas.xml'</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sbas.xml', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>3. Run GIAnT</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We have now created all of the necessary files to run GIAnT. The full GIAnT process requires 3 function calls.\n",
    "<ul>\n",
    "    <li>PrepIgramStack.py</li>\n",
    "    <li>ProcessStack.py</li>\n",
    "    <li>SBASInvert.py</li>\n",
    "</ul>\n",
    "We will make a 4th function call that is not necessary, but provides some error estimation that can be useful.\n",
    "<ul>\n",
    "    <li>SBASxval.py</li>\n",
    "</ul>\n",
    "    </font>\n",
    "    \n",
    "<font face='Calibri'> <font size='4'> <b>3.0 Run PrepIgramStack.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> Here we run <b>PrepIgramStack.py</b> to create the files for GIAnT. This will read in the input data and the files we previously created. This will output HDF5 files. As we did not have to modify <b>PrepIgramStack.py</b>, we will call from the installed GIAnT library. <br>\n",
    "Inputs:       \n",
    "- ifg.list\n",
    "- data.xml\n",
    "- sbas.xml        \n",
    "\n",
    "Outputs:\n",
    "- RAW-STACK.h5\n",
    "- PNG previews under 'Igrams' folder \n",
    "    \n",
    "    </font> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'><b>3.0.0 Display the help information for PrepIgramStack.py</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/PrepIgramStack.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>3.0.1 Run PrepIgramStack.py</b>\n",
    "<br>\n",
    "Note: The processing status bar may not fully complete when the script is finished running. This is okay.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/PrepIgramStack.py -i ifg.list\n",
    "# The '-i ifg.list' is technically unnecessary as that is the default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>3.0.2 Confirm the correct HDF5 file format</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join('Stack','RAW-STACK.h5')\n",
    "if not h5py.is_hdf5(file):\n",
    "    print(f'Not an HDF5 file:{file}')\n",
    "else:\n",
    "    print(\"It's an HDF5 file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>3.1 Run ProcessStack.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> This seems to be an optional step. Does atmospheric corrections and estimation of orbit residuals. <br>\n",
    "Inputs:\n",
    "\n",
    "- HDF5 files from PrepIgramStack.py, RAW-STACK.h5\n",
    "- data.xml \n",
    "- sbas.xml\n",
    "- GPS Data (optional; we don't have this)\n",
    "- Weather models (downloaded automatically)\n",
    "\n",
    "Outputs: \n",
    "\n",
    "- HDF5 files, PROC-STACK.h5\n",
    "        \n",
    "These files are then fed into SBAS. \n",
    "</font> </font>\n",
    "<br><br>\n",
    "<font face='Calibri' size='3'><b>3.1.0 Display the help information for ProcessStack.py</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/ProcessStack.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>3.1.1 Run ProcessStack.py</b>\n",
    "<br>\n",
    "Note: The processing status bars may never fully complete. This is okay.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/ProcessStack.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>3.1.2 Confirm the correct HDF5 file format</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join('Stack','PROC-STACK.h5')\n",
    "#print(files)\n",
    "if not h5py.is_hdf5(file):\n",
    "    print(f'Not an HDF5 file:{file}')\n",
    "else:\n",
    "    print(\"It's an HDF5 file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>3.2 Run SBASInvert.py </b> </font>\n",
    "<br>\n",
    "<font size='3'> Run the time series. \n",
    "Inputs\n",
    "<ul>\n",
    "    <li>HDF5 file, PROC-STACK.h5</li>\n",
    "    <li>data.xml</li>\n",
    "    <li>sbas.xml</li>\n",
    "</ul>\n",
    "Outputs\n",
    "<ul>\n",
    "    <li>HDF5 file: LS-PARAMS.h5</li>\n",
    "</ul>\n",
    "\n",
    "</font> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'><b>3.2.0 Display the help information for SBASInvert.py</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/SBASInvert.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>3.2.1 Run SBASInvert.py</b>\n",
    "<br><br>\n",
    "Note: The processing status bar may not fully complete when the script is finished running. This is okay.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/SBASInvert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'> <font size='4'> <b>3.3 Run SBASxval.py </b> </font>\n",
    "<br>\n",
    "    <font size='3'> Get an uncertainty estimate for each pixel and epoch using a Jacknife test. This is often the most time consuming portion of the code. \n",
    " \n",
    "Inputs: \n",
    "<ul>\n",
    "    <li>HDF5 files, PROC-STACK.h5</li>\n",
    "    <li>data.xml</li>\n",
    "    <li>sbas.xml</li>\n",
    "</ul>\n",
    "Outputs:\n",
    "<ul>\n",
    "    <li>HDF5 file, LS-xval.h5</li>\n",
    "        </ul>\n",
    "</font> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'><b>3.3.0 Display the help information for SBASxval.py</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/SBASxval.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>3.3.1 Run SBASInvert.py</b>\n",
    "<br><br>\n",
    "Note: The processing status bar may not fully complete when the script is finished running. This is okay.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $giant_path/SBASxval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>3.4 Having completed our GIAnT processing, return to the analysis' analysis_directory directory</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(analysis_directory)\n",
    "print(f'Current directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>4. Data Visualization</b></font>\n",
    "<br><br>\n",
    "<font size='4'><b>4.0 Prepare data for visualization</b></font>  \n",
    "<br>\n",
    "<font size='3'><b>4.0.0 Load the LS-PARAMS stack produced by GIAnT and read it into an array so we can manipulate and display it.</b></font></font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(giant_dir, 'Stack/LS-PARAMS.h5')\n",
    "f = h5py.File(filename, 'r')\n",
    "# List all groups\n",
    "group_key = list(f.keys()) # get list of all keys. \n",
    "print(f\"group_key: {group_key}\") # print list of keys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.0.1 Display LS-PARAMS info with gdalinfo</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalinfo {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>4.0.2 Load the LS-xval stack produced by GIAnT and read it into an array so we can manipulate and display it.</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(giant_dir, 'Stack/LS-xval.h5')\n",
    "error = h5py.File(filename, 'r')\n",
    "# List all groups\n",
    "error_key = list(error.keys()) # get list of all keys. \n",
    "print(f\"error_key: {error_key}\") # print list of keys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.0.3 Display LS-xval info with gdalinfo</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdal.Info(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Details on what each of these keys in group_key and error_key means can be found in the GIAnT documentation. For now, the only keys with which we are concerned are 'recons' (the filtered time series of each pixel) and 'dates' (the dates of acquisition). It is important to note that the dates are given in a type of Julian Day number called Rata Die number which we convert to Gregorian dates below. </font></font>\n",
    "<br><br>\n",
    "<font face='Calibri'><font size='3'><b>4.0.4 Get the relevant data and error values from our stacks and confirm that the number of rasters in our data stack, the number of error points, and the number of dates are all the same.</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get our data from the stack\n",
    "data_cube = f['recons'][()]\n",
    "\n",
    "# Get our error from the stack\n",
    "error_cube = error['error'][()]\n",
    "\n",
    "# Get the dates for each raster from the stack\n",
    "dates = list(f['dates']) # these dates appear to be given in Rata Die style: floor(Julian Day Number - 1721424.5). \n",
    "\n",
    "# Check that the number of rasters in the data cube, error cube, and the number of dates are the same. \n",
    "if data_cube.shape[0] is not len(dates) or error_cube.shape[0] is not len(dates):\n",
    "    print('Problem')\n",
    "    print('Number of rasters in data_cube: ', data_cube.shape[0])\n",
    "    print('Number of rasters in error_cube: ', error_cube.shape[0])\n",
    "    print('Number of dates: ', len(dates))\n",
    "else:\n",
    "    print('The number of dates and rasters match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>4.0.5 Convert from Rata Die number (similar to Julian Day number) contained in 'dates' to Gregorian date. </b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tindex = []\n",
    "for d in dates:\n",
    "    tindex.append(date.fromordinal(int(d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>4.0.6 Create an amplitude image of the volcano.</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar = gdal.Open(radar_tiff) # open my radar tiff using GDAL\n",
    "im_radar = radar.GetRasterBand(1).ReadAsArray() # access the band information\n",
    "dbplot = np.ma.log10(im_radar) # plot as a log10\n",
    "dbplot[isneginf(dbplot)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>4.1 Select a Point-of-Interest</b></font>\n",
    "    <br><br>\n",
    "    <font size='3'><b>4.1.0 Write a class to create an interactive matplotlib plot that displays an overlay of the clipped deformation map and amplitude image, and allows the user to select a point of interest.</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pixelPicker:\n",
    "    def __init__(self, image1, image2, width, height):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.fig = plt.figure(figsize=(width, height), tight_layout=True)\n",
    "        self.fig.tight_layout()\n",
    "        self.ax = self.fig.add_subplot(111, visible=False)\n",
    "        self.rect = patches.Rectangle(\n",
    "            (0.0, 0.0), width, height, \n",
    "            fill=False, clip_on=False, visible=False,\n",
    "        )\n",
    "        self.rect_patch = self.ax.add_patch(self.rect)\n",
    "        self.cid = self.rect_patch.figure.canvas.mpl_connect(\n",
    "            'button_press_event',\n",
    "            self\n",
    "        )\n",
    "        self.image1 = image1\n",
    "        self.image2 = image2        \n",
    "        self.plot = self.defNradar_plot(self.image1, self.image2, self.fig, return_ax=True)\n",
    "        self.plot.set_title('Select a Point of Interest from db-Scaled Amplitude Image')\n",
    "        \n",
    "        \n",
    "    def defNradar_plot(self, deformation, radar, fig, return_ax=False):\n",
    "        ax = fig.add_subplot(111)\n",
    "        vmin = np.percentile(radar, 3)\n",
    "#         vmax = np.percentile(radar, 97)\n",
    "        vmax = np.percentile(radar, 81) # 81 is the max number. any bigger numbers will cause an error.\n",
    "\n",
    "        im = ax.imshow(radar, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        vmin = np.percentile(deformation, 3)\n",
    "        vmax = np.percentile(deformation, 97)\n",
    "        fin_plot = ax.imshow(deformation, cmap='RdBu', vmin=vmin, vmax=vmax, alpha=0.75)\n",
    "        cbar = fig.colorbar(fin_plot, fraction=0.24, pad=0.02)\n",
    "        cbar.ax.set_xlabel('mm')\n",
    "        cbar.ax.set_ylabel('Deformation', rotation=270, labelpad=20)\n",
    "        #ax.set(title=\"Integrated Defo [mm] Overlaid on Clipped db-Scaled Amplitude Image\")\n",
    "        plt.grid()\n",
    "        if return_ax:\n",
    "            return(ax)\n",
    "    \n",
    "    \n",
    "    def __call__(self, event):\n",
    "        print('click', event)\n",
    "        self.x = event.xdata\n",
    "        self.y = event.ydata\n",
    "        for pnt in self.plot.get_lines():\n",
    "            pnt.remove()\n",
    "        plt.plot(self.x, self.y, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>4.1.1 Create the interactive point of interest plot, and select a point</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_xsize = 9\n",
    "fig_ysize = 5\n",
    "deformation = data_cube[data_cube.shape[0]-1]\n",
    "my_plot = pixelPicker(deformation, dbplot, fig_xsize, fig_ysize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.1.2 Grab the line and pixel for the selected POI</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_line = ceil(my_plot.y)\n",
    "my_pixel = ceil(my_plot.x)\n",
    "print(f'my_line: {my_line}')\n",
    "print(f'my_pixel: {my_pixel}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>4.2 Visualize the data for the selected POI</b></font>\n",
    "    <br><br>\n",
    "    <font size='3'><b>4.2.0 Write a function to visualize the time series for a single pixel (POI) as well as the associated error for that pixel.</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_with_error(deformation, radar, series_d, \n",
    "                           series_e, my_line, my_pixel, \n",
    "                           tindex, fig_xsize, fig_ysize):\n",
    "    # Create base figure; 2 rows, 1 column\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1,\n",
    "                             figsize=(fig_xsize,fig_ysize*2),\n",
    "                             gridspec_kw={'width_ratios':[1],'height_ratios':[1,1]})\n",
    "    # Plot radar image with deformation in left column with crosshair over pixel of interest\n",
    "    vmin = np.percentile(radar, 3)\n",
    "#     vmax = np.percentile(radar, 97)\n",
    "    vmax = np.percentile(radar, 81) # same issue... any number bigger than 81 will cause an error\n",
    "    axes[0].imshow(radar, cmap='gray', vmin=vmin, vmax=vmax, aspect='equal')\n",
    "    vmin = np.percentile(deformation, 3)\n",
    "    vmax = np.percentile(deformation, 97)\n",
    "    fin_plot = axes[0].imshow(deformation, cmap='RdBu', vmin=vmin, vmax=vmax, alpha=0.75, aspect='equal')\n",
    "    cbar = fig.colorbar(fin_plot, fraction=0.24, pad=0.02, ax=axes[0])\n",
    "    cbar.ax.set_xlabel('mm')\n",
    "    cbar.ax.set_ylabel('Deformation', rotation=270, labelpad=20)\n",
    "    axes[0].set(title=\"Integrated Defo [mm] Overlain on\\nClipped db-Scaled Amplitude Image\")\n",
    "    axes[0].grid() # overlay a grid on the image. \n",
    "    \n",
    "    # Put crosshair over POI in first figure. \n",
    "    axes[0].plot(my_pixel, my_line, 'cx', markersize=12, markeredgewidth=4) # create a cyan x ('cx') at location [myPixel,myLine]\n",
    "    axes[0].legend(labels=['POI']) # Put in a legend with the label 'POI'\n",
    "    \n",
    "    # Plot time series of pixel in right column with error bars\n",
    "    # Make pandas time series object\n",
    "    ts_d = pd.Series(series_d, index=tindex)\n",
    "    # Plot the time series object with the associated error. \n",
    "    ts_d.plot(marker='.', markersize=6, yerr=series_e)\n",
    "    axes[1].legend(labels=[f'Time Series for POI at ({my_line}, {my_pixel})'])\n",
    "    axes[1].set(title=\"Integrated Defo [mm] with Error Estimates for a Single Pixel\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Deformation [mm]')\n",
    "    for tick in axes[1].get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "    plt.grid() # overlay a grid on the time series plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>4.2.1 Plot the time series for the selected POI</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_d = data_cube[:, my_line, my_pixel]\n",
    "series_e = error_cube[:, my_line, my_pixel]\n",
    "\n",
    "time_series_with_error(deformation, dbplot, series_d, \n",
    "                       series_e, my_line, my_pixel, \n",
    "                       tindex, fig_xsize, fig_ysize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>4.2.2 Create an animation of the deformation</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "ax = fig.add_subplot(111)\n",
    "#ax.axis('off')\n",
    "vmin=np.percentile(data_cube.flatten(), 5)\n",
    "vmax=np.percentile(data_cube.flatten(), 95)\n",
    "\n",
    "\n",
    "vmin = np.percentile(deformation, 3)\n",
    "vmax = np.percentile(deformation, 97)\n",
    "im = ax.imshow(data_cube[0], cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "ax.set_title(\"Animation of Deformation Time Series - Sierra Negra, Galapagos\")\n",
    "cbar = fig.colorbar(im)\n",
    "cbar.ax.set_xlabel('mm')\n",
    "cbar.ax.set_ylabel('Deformation', rotation=270, labelpad=20)\n",
    "plt.grid()\n",
    "\n",
    "def animate(i):\n",
    "    ax.set_title(\"Date: {}\".format(tindex[i]))\n",
    "    im.set_data(data_cube[i])\n",
    "    \n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=data_cube.shape[0], interval=400)\n",
    "rc('animation', embed_limit=10.0**9) # set the maximum animation size allowed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>4.2.3 Display the animation</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>Define a prefix to prepend to all output files for identification purposes</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    prefix = input(\"Enter an identifiable prefix to prepend to output files: \")\n",
    "    if re.search('\\W', prefix):\n",
    "        print(f\"\\n\\\"{prefix}\\\" contains invalid characters. Please try again.\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>4.2.5 Save the animation as a gif</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani_path = os.path.join(output_dir, f\"{prefix}_filt={filt}_{date.today()}.gif\")\n",
    "ani.save(ani_path, writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Note: 'date.today()' returns the current date from the system on which it is called. As OpenSAR Lab runs in machines on the East coast of the United States, date.today() will return the date based on Eastern Standard Time (EST).</font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>5. HDF5 TO GEOTIFF</b></font>\n",
    "    <br>\n",
    "    <font size='3'>Convert the HDF5 file to a series of geolocated geotiffs. This method comes from an answer to this <a href=\"https://gis.stackexchange.com/questions/37238/writing-numpy-array-to-raster-file\" target=\"_blank\">StackExchange question</a> which is based on a recipe from the <a href=\"https://pcjericks.github.io/py-gdalogr-cookbook/raster_layers.html#create-raster-from-array\">Python GDAL/OGR Cookbook</a>.</font>\n",
    "    <br>\n",
    "<font size='4'>  \n",
    "<b>5.0 Display the size of the data-stack</b> \n",
    "</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rasters: {data_cube.shape[0]}\")\n",
    "print(f\"Height & Width in Pixels: {data_cube.shape[1]}, {data_cube.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.1 Get raster information from one of the subsets.</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_corr[0])\n",
    "src = gdal.Open(files_corr[0])\n",
    "geo_transform = src.GetGeoTransform()\n",
    "del src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.2  Write a function to convert a numpy array into a geotiff</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_geotiff(in_raster, geo_transform, array, epsg):\n",
    "    cols = array.shape[1]\n",
    "    rows = array.shape[0]\n",
    "\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_raster = driver.Create(in_raster, cols, rows, 1, gdal.GDT_Float32)\n",
    "    out_raster.SetGeoTransform(geo_transform)\n",
    "    outband = out_raster.GetRasterBand(1)\n",
    "    outband.WriteArray(array)\n",
    "    out_raster_srs = osr.SpatialReference()\n",
    "    out_raster_srs.ImportFromEPSG(epsg)\n",
    "    out_raster.SetProjection(out_raster_srs.ExportToWkt())\n",
    "    outband.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.3 Create the geotiffs</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, data_cube.shape[0]):\n",
    "    try:\n",
    "        geotiff_path = os.path.join(output_dir, f\"{prefix}_{tindex[i]}.tif\")\n",
    "        print(f\"{i}: {geotiff_path}\")\n",
    "        array = data_cube[i, :, :].copy()\n",
    "        array_to_geotiff(geotiff_path, geo_transform, array, int(part1_pickle['utm']))\n",
    "        print(f\"Geotiff {i+1} of {data_cube.shape[0]} created\\nFile path: {geotiff_path}\")\n",
    "    except:\n",
    "        print(f\"Geotiff {i+1} of {data_cube.shape[0]} FAILED\\nFile path: {geotiff_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>6. Alter the time filter parameter</b></font><br>\n",
    "    <font size='3'>Looking at the video above, you may notice that the deformation has a very smoothed appearance. This may be because of our time filter which is currently set to 1 year ('filt=1.0' in the prepxml_SBAS.py code). Let's repeat the lab from there with 2 different time filters. <br>First, using no time filter ('filt=0.0') and then using a 1 month time filter ('filt=0.082'). Change the output filename prefix for anything you want saved (e.g., 'SierraNegraDeformationTS.gif' to 'YourDesiredFileName.gif'). Otherwise, it will be overwritten. <br>\n",
    "<ul>\n",
    "    <li>How did these changes affect the output time series?</li>\n",
    "    <li>How might we figure out the right filter length?</li>\n",
    "    <li>What does this say about the parameters we select?</li>\n",
    "</ul>\n",
    "</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>7. Clear data from the Notebook (optional)</b></font>\n",
    "    <br>\n",
    "    <font size='3'>This lab has produced a large quantity of data. If you look at this notebook in your analysis_directory directory, it should now be ~80 MB. This can take a long time to load in a Jupyter Notebook. It may be useful to clear the cell outputs, which will restore the Notebook to its original size. <br>To clear the cell outputs, go Cell->All Output->Clear. This will clear the outputs of the Jupyter Notebook and restore it to its original size of ~60 kB. This will not delete any of the files we have created. </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-success\">\n",
    "<font face=\"Calibri\" size=\"5\"> <b> <font color='rgba(200,0,0,0.2)'> <u>ASSIGNMENT ##</u>:  </font> Explore the effect of Atmospheric Correction  </b> <font color='rgba(200,0,0,0.2)'> -- [## Points] </font> </font>\n",
    "\n",
    "<font face=\"Calibri\" size=\"3\"> <u>Compare corrected and uncorrected time series for selected pixels</u>:\n",
    "<ol type=\"1\">\n",
    "    <li>Repeat the time series using the original subsets (i.e., those that TRAIN did not correct). MAKE SURE TO SAVE IT AS A DIFFERENT FILE. Hint: userfn.py will need to be modified. <font color='rgba(200,0,0,0.2)'> -- [## Point] </font></li>\n",
    "    <br>\n",
    "    <li>Show a plot of deformation through time for 3 selected pixels. Each plot should have the corrected and uncorrected time series for the selected pixels. Makes sure to select at least 1 point near the summit of the volcano (where deformation should be the greatest) and 1 point at low elevation (where deformation should be relatively small). The location of the third point is at your discretion. Hint: the variables 'myLine' and 'myPixel' can be made into a pair of lists over which a loop is run. <font color='rgba(200,0,0,0.2)'> -- [## Points] </font></li> \n",
    "</ol> \n",
    "</font>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657-Lab9-InSARTimeSeriesAnalysis-Part2-GIAnT.ipynb - Version 3.1.0 - November 2021\n",
    "    <br>\n",
    "        <b>Version Changes:</b>\n",
    "    <ul>\n",
    "        <li>asf_notebook -> opensarlab_lib</li>\n",
    "        <li>url_widget</li>\n",
    "        <li>%matplotlib notebook -> %matplotlib widget</li>\n",
    "    </ul>\n",
    "    </i>\n",
    "</font>\n",
    "    \n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtc_analysis [conda env:.local-rtc_analysis]",
   "language": "python",
   "name": "conda-env-.local-rtc_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
